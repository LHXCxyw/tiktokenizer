// åœ¨å¯¼å…¥ @xenova/transformers ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡
if (typeof window === "undefined") {
  // æœåŠ¡å™¨ç¯å¢ƒï¼šç¦ç”¨ç¼“å­˜ç›¸å…³åŠŸèƒ½ä»¥é¿å…æ–‡ä»¶ç³»ç»Ÿå†™å…¥é”™è¯¯
  process.env.TRANSFORMERS_OFFLINE = "1";
  process.env.HF_HUB_DISABLE_TELEMETRY = "1";
}

import { hackModelsRemoveFirstToken } from "./index";
import { get_encoding, encoding_for_model, type Tiktoken } from "tiktoken";
import { oaiEncodings, oaiModels, openSourceModels } from ".";
import { PreTrainedTokenizer, env } from "@xenova/transformers";
import type { z } from "zod";
import {
  getHuggingfaceSegments,
  getTiktokenSegments,
  type Segment,
} from "~/utils/segments";

// Lambda å†…å­˜ç¼“å­˜ï¼šç¼“å­˜å·²åŠ è½½çš„ tokenizer å¯¹è±¡
const tokenizerCache = new Map<string, Tokenizer>();

export interface TokenizerResult {
  name: string;
  // Array<{ text: string; tokens: { id: number; idx: number }[] }> ?
  tokens: number[];
  segments?: Segment[];
  count: number;
}

export interface Tokenizer {
  name: string;
  tokenize(text: string): TokenizerResult;
  free?(): void;
}

export class TiktokenTokenizer implements Tokenizer {
  private enc: Tiktoken;
  name: string;
  constructor(model: z.infer<typeof oaiModels> | z.infer<typeof oaiEncodings>) {
    const isModel = oaiModels.safeParse(model);
    const isEncoding = oaiEncodings.safeParse(model);
    console.log(isModel.success, isEncoding.success, model)
    if (isModel.success) {

      if (
        model === "text-embedding-3-small" ||
        model === "text-embedding-3-large"
      ) {
        throw new Error("Model may be too new");
      }

      const enc =
        model === "gpt-3.5-turbo" || model === "gpt-4" || model === "gpt-4-32k"
          ? get_encoding("cl100k_base", {
            "<|im_start|>": 100264,
            "<|im_end|>": 100265,
            "<|im_sep|>": 100266,
          })
          : model === "gpt-4o"
            ? get_encoding("o200k_base", {
              "<|im_start|>": 200264,
              "<|im_end|>": 200265,
              "<|im_sep|>": 200266,
            })
            : // @ts-expect-error r50k broken?
            encoding_for_model(model);
      this.name = enc.name ?? model;
      this.enc = enc;
    } else if (isEncoding.success) {
      this.enc = get_encoding(isEncoding.data);
      this.name = isEncoding.data;
    } else {
      throw new Error("Invalid model or encoding");
    }
  }

  tokenize(text: string): TokenizerResult {
    const tokens = [...(this.enc?.encode(text, "all") ?? [])];
    return {
      name: this.name,
      tokens,
      segments: getTiktokenSegments(this.enc, text),
      count: tokens.length,
    };
  }

  free(): void {
    this.enc.free();
  }
}

export class OpenSourceTokenizer implements Tokenizer {
  constructor(private tokenizer: PreTrainedTokenizer, name?: string) {
    this.name = name ?? tokenizer.name;
  }

  name: string;

  static async load(
    model: z.infer<typeof openSourceModels>,
    hostInfo?: string
  ): Promise<PreTrainedTokenizer> {
    // ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„ä¸»æœºä¿¡æ¯ï¼Œå¦‚æœæä¾›çš„è¯
    if (hostInfo) {
      console.log(`ä½¿ç”¨å¤–éƒ¨æä¾›çš„ä¸»æœºä¿¡æ¯: ${hostInfo}`);
      env.remoteHost = hostInfo;
    } else if (typeof window !== "undefined") {
      // æµè§ˆå™¨ç¯å¢ƒï¼šä½¿ç”¨å½“å‰é¡µé¢çš„origin
      console.log("æµè§ˆå™¨ç¯å¢ƒï¼Œä½¿ç”¨å½“å‰é¡µé¢origin");
      env.remoteHost = window.location.origin;
    } else {
      // æœåŠ¡å™¨ç¯å¢ƒä½†æœªæä¾›ä¸»æœºä¿¡æ¯ï¼šä½¿ç”¨é»˜è®¤è¡Œä¸º
      console.log("æœåŠ¡å™¨ç¯å¢ƒä¸”æœªæä¾›ä¸»æœºä¿¡æ¯ï¼Œå°†ä½¿ç”¨é»˜è®¤è¿œç¨‹ä¸»æœº");
    }
    env.remotePathTemplate = "/hf/{model}";
    // åœ¨æœåŠ¡å™¨ç¯å¢ƒä¸­é…ç½®ç¼“å­˜ç­–ç•¥
    if (typeof window === "undefined") {
      console.log("æœåŠ¡å™¨ç¯å¢ƒï¼šé…ç½®æ— ç¼“å­˜æ¨¡å¼");
      // å®Œå…¨ç¦ç”¨ç¼“å­˜ä»¥é¿å…æ–‡ä»¶ç³»ç»Ÿå†™å…¥é”™è¯¯
      env.useBrowserCache = false;
      env.allowLocalModels = false;
    }
    const t = await PreTrainedTokenizer.from_pretrained(model, {
      // æœ€å°åŒ–æ—¥å¿—è¾“å‡ºï¼šåªåœ¨å…³é”®çŠ¶æ€æ—¶è®°å½•
      progress_callback: (progress: any) => {
        if (progress.status === 'initiate') {
          console.log(`å¼€å§‹åŠ è½½ "${model}" ${progress.file}`);
        } else if (progress.status === 'done') {
          console.log(`å®ŒæˆåŠ è½½ "${model}" ${progress.file}`);
        }
        // å¿½ç•¥æ‰€æœ‰ 'download' å’Œ 'progress' çŠ¶æ€ä»¥å‡å°‘æ—¥å¿—
      },
      // ç¦ç”¨æœ¬åœ°æ–‡ä»¶ä¼˜å…ˆæœºåˆ¶ï¼Œå¼ºåˆ¶ä»è¿œç¨‹ä¸‹è½½
      local_files_only: false,
    });
    console.log("loaded tokenizer", model, t.name);
    return t;
  }

  tokenize(text: string): TokenizerResult {
    // const tokens = this.tokenizer(text);
    const tokens = this.tokenizer.encode(text);
    const removeFirstToken = (
      hackModelsRemoveFirstToken.options as string[]
    ).includes(this.name);
    return {
      name: this.name,
      tokens,
      segments: getHuggingfaceSegments(this.tokenizer, text, removeFirstToken),
      count: tokens.length,
    };
  }
}

export async function createTokenizer(
  name: string,
  options?: { hostInfo?: string }
): Promise<Tokenizer> {
  console.log("createTokenizer", name, options?.hostInfo ? `with hostInfo: ${options.hostInfo}` : "without hostInfo");

  // ğŸš€ æ£€æŸ¥ Lambda å†…å­˜ç¼“å­˜ï¼šå¦‚æœå·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›ï¼ˆæ¯«ç§’çº§é€Ÿåº¦ï¼‰
  const cacheKey = `${name}_${options?.hostInfo || 'default'}`;
  if (tokenizerCache.has(cacheKey)) {
    console.log(`ğŸš€ ä»ç¼“å­˜è¿”å› tokenizer: ${name} (è¶…å¿«é€Ÿåº¦)`);
    return tokenizerCache.get(cacheKey)!;
  }

  const oaiEncoding = oaiEncodings.safeParse(name);
  if (oaiEncoding.success) {
    console.log("oaiEncoding", oaiEncoding.data);
    const tokenizer = new TiktokenTokenizer(oaiEncoding.data);
    tokenizerCache.set(cacheKey, tokenizer);
    return tokenizer;
  }
  const oaiModel = oaiModels.safeParse(name);
  if (oaiModel.success) {
    console.log("oaiModel", oaiModel.data);
    const tokenizer = new TiktokenTokenizer(oaiModel.data);
    tokenizerCache.set(cacheKey, tokenizer);
    return tokenizer;
  }

  const ossModel = openSourceModels.safeParse(name);
  if (ossModel.success) {
    console.log("loading tokenizer", ossModel.data);
    const huggingfaceTokenizer = await OpenSourceTokenizer.load(ossModel.data, options?.hostInfo);
    console.log("loaded tokenizer", name);
    const tokenizer = new OpenSourceTokenizer(huggingfaceTokenizer, name);

    // ğŸ’¾ ç¼“å­˜åˆ° Lambda å†…å­˜ï¼Œåç»­è°ƒç”¨å°†è¾¾åˆ°å‰ç«¯ä¸€æ ·çš„è¶…å¿«é€Ÿåº¦
    tokenizerCache.set(cacheKey, tokenizer);
    console.log(`ğŸ’¾ å·²ç¼“å­˜ tokenizer: ${name} - åç»­è°ƒç”¨å°†è¶…å¿«`);

    return tokenizer;
  }
  throw new Error("Invalid model or encoding");
}
